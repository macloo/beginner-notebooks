{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A First Web Scraping Exercise\n",
    "\n",
    "We're going to scrape a bunch of separate lists from ONE Wikipedia page. Wikipedia is a good place to practice scraping because the HTML there is (mostly) well formatted, and the site's traffic is so high, they don't mind us hitting one page again and again. \n",
    "\n",
    "First we import two libraries: \n",
    "* [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is for extracting what we want from a page or pages.\n",
    "* [Requests](http://docs.python-requests.org/en/master/user/quickstart/) is for making the HTTP request to the server where we want to scrape.\n",
    "\n",
    "**If you have not yet installed these two libraries** into the virtual environment where you are running this Notebook, you'll need to do that *first.*\n",
    "\n",
    "This is the page we will scrape: [List of colleges and universities in Florida](https://en.wikipedia.org/wiki/List_of_colleges_and_universities_in_Florida)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Python libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most scraping scripts begin with one page, which means one URL, to be scraped. \n",
    "\n",
    "In the three lines below, the only thing that changes if you scrape a different page, even at another website, is the URL inside the single quotaton marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the page and copy all its HTML into a variable named `soup`\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_colleges_and_universities_in_Florida'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of those three lines above, we now have a BeautifulSoup \"object\" named `soup`, and it contains **all the HTML** from the file at the URL we provided. THe HTML is stored in a manner that allows the BeautifulSoup functions to search and use the data.\n",
    "\n",
    "Our goal is to scrape **all the lists** of colleges from that one Wikipedia page and put them in a CSV file, with separate columns for college name, location, type of college, and the URL for that college's Wikipedia page.\n",
    "\n",
    "We start by using Chrome Dev Tools: right-click on the heading \"State University System\" and select **Inspect** from the pop-up menu.\n",
    "\n",
    "If we inspect the heading above *each* of the lists of colleges and schools, we find that *all* of those headings are H3 headings. \n",
    "\n",
    "\n",
    "## Find all of the H3 elements\n",
    "\n",
    "Here comes your first Beautiful Soup command &mdash;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by collecting all the H3 elements on the page into a Python list\n",
    "h3_list = soup.find_all('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't really NEED to print them, but we do it so we can see what we got\n",
    "# I would delete this cell after I had run it - it's only for testing \n",
    "print(h3_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look closely, and you'll see that all the H3 heading elements are items in a Python *list.* A Python list is enclosed in square brackets &mdash; `[ ]`\n",
    "\n",
    "We're going to use a heading to grab the list of schools that follows it.\n",
    "\n",
    "## Grab a list of schools that follows a certain H3 heading\n",
    "\n",
    "The heading above the first list of schools we want is \"State University System\" &mdash; so let's get the first UL that comes after that heading. (A UL element in HTML is an \"unordered list,\" which is a list with bullets.)\n",
    "\n",
    "But first, notice how there are multiple SPAN elements inside the H3 element.\n",
    "\n",
    "Then look at the code we use to grab the UL (the list of schools):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for head in h3_list:\n",
    "    if head.span.text == 'State University System':\n",
    "        the_ul = head.find_next('ul')\n",
    "        print(the_ul)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, printing all of that *wasn't strictly necessary,* but it's good to make sure you got what you were trying to get. Otherwise, if you start getting error messages, you won't understand why. \n",
    "\n",
    "**Note that the code that enables us to print the list *will be used later* to scrape the list.** Printing is not the point of the scrape. It's just a step along the way.\n",
    "\n",
    "You should be able to recognize that you got the complete UL element and everything it contains. It is NOT in a Python list. It is one single BeautifulSoup \"tag object,\" and it's been assigned to the **variable** named `the_ul`.\n",
    "\n",
    "* Above, you began with a *list* of all H3 elements on the page. Each H3 element from a Wikipedia page contains `span` tags and text and more. (You saw this with **Inspect.**)\n",
    "\n",
    "* The next task was to *get* the complete UL element that comes after the heading we chose: \"State University System.\" So we *looped* over the list of all H3 elements, and we checked *each one* to see if the text inside its *first* `span` tag matched the string we provided.\n",
    "\n",
    "* As soon as we found that exact string, we grabbed *the next UL element* that comes after that H3, and we assigned that entire UL element to the variable `the_ul`.\n",
    "\n",
    "* We printed that whole lump of code.\n",
    "\n",
    "* Then, we stopped. The command `break` makes the for-loop stop looping.\n",
    "\n",
    "\n",
    "##  Get the info we want for *each* school in the list\n",
    "\n",
    "Next, we want to get **three things** out of each LI element in that list: the college name, the location, and the URL for that college's Wikipedia page. Use **Inspect** again to see the structure of the HTML and figure out how to use it.\n",
    "\n",
    "We will scrape the **text** for college name and location. We will scrape the **href** (inside the A element) for the URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a Python list of all items in this UL element (in `the_ul`)\n",
    "schools = the_ul.find_all('li')\n",
    "\n",
    "# loop over that list, scraping three things from each line in the list\n",
    "for li in schools:\n",
    "    a_list = li.find_all('a')\n",
    "    college_name = a_list[0].text\n",
    "    location = a_list[1].text\n",
    "    url = a_list[0]['href']\n",
    "    print(college_name, location, url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the code above does\n",
    "\n",
    "One by one, for one LI at a time, we:\n",
    "\n",
    "* Find all the A elements in it and put them into a new list.\n",
    "* From the *first* A element, `a_list[0]`, we get the text inside the A tags and assign it to the variable `college_name`.\n",
    "* From the *second* A element, `a_list[1]`, we get the text inside the A tags and assign it to the variable `location`.\n",
    "* From the *first* A element, `a_list[0]`, we get the *value* of the `href` attribute and assign it to the variable `url`.\n",
    "* We print the three variables for each time the loop loops.\n",
    "\n",
    "### How to make the partial URL a complete URL\n",
    "\n",
    "This is another common scraping task. The HTML holds a partial URL because the link goes to another page on Wikipedia. We want a full URL so we can use it *outside* Wikipedia.\n",
    "\n",
    "The solution is simply to *add* the missing front part of the URL to the partial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the missing front part of the URL is - 'https://en.wikipedia.org'\n",
    "# let's re-do the same for-loop from above\n",
    "\n",
    "for li in schools:\n",
    "    a_list = li.find_all('a')\n",
    "    college_name = a_list[0].text\n",
    "    location = a_list[1].text\n",
    "    # here, we add the missing front part of the URL \n",
    "    url = 'https://en.wikipedia.org' + a_list[0]['href']\n",
    "    print(college_name, location, url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put what was scraped into a CSV file\n",
    "\n",
    "Python has standard code for doing this. You will use these few lines of code the same way every time you need to store data in a CSV.\n",
    "\n",
    "To demonstrate, we'll create a \"test\" CSV file with two rows of nonsense data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Python's built-in CSV module\n",
    "import csv\n",
    "\n",
    "# create and open a new file for writing - its name will be `test.csv` \n",
    "csvfile = open(\"test.csv\", 'w', newline='', encoding='utf-8')\n",
    "\n",
    "# make a new variable, c, for Python's CSV writer object \n",
    "c = csv.writer(csvfile)\n",
    "\n",
    "# write your header row to test.csv\n",
    "c.writerow( ['first', 'second', 'third', 'fourth'] )\n",
    "\n",
    "# write two more rows to test.csv\n",
    "c.writerow( ['a', 'b', 'c', 'd'] )\n",
    "c.writerow( [10, 20, 30, 40] )\n",
    "\n",
    "# close and save the CSV file\n",
    "csvfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now created a new CSV file and written three rows and four columns into it. But WHERE was it *saved*? To find out, use the `pwd` to find out which directory this Jupyter Notebook is running in. (`pwd` is a command that stands for \"print working directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to that folder now, in your Finder or File Explorer, and open the CSV file named *test.csv.*\n",
    "\n",
    "Now you know **how to write a new CSV file.** Can you figure out how to take the earlier code from above &mdash; the for-loop that PRINTS a list of schools &mdash; and make it WRITE to a row in the CSV *instead* of printing?\n",
    "\n",
    "Because that's all you need to do.\n",
    "\n",
    "Note: The `import csv` line does not need to be repeated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and open a new file for writing - its name will be `test.csv` \n",
    "csvfile = open(\"state_university_system.csv\", 'w', newline='', encoding='utf-8')\n",
    "\n",
    "# make a new variable, c, for Python's CSV writer object\n",
    "c = csv.writer(csvfile)\n",
    "\n",
    "# write your header row to the file \n",
    "c.writerow( ['school', 'location', 'url'] )\n",
    "\n",
    "# the same code from above that loops over all the schools in the list of LI elements\n",
    "for li in schools:\n",
    "    a_list = li.find_all('a')\n",
    "    college_name = a_list[0].text\n",
    "    location = a_list[1].text\n",
    "    url = 'https://en.wikipedia.org' + a_list[0]['href']\n",
    "    # instead of print() --- we write to the CSV file \n",
    "    c.writerow( [college_name, location, url] )\n",
    "\n",
    "# close and save the CSV file\n",
    "csvfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open your new CSV, named *state_university_system.csv,* and take a look at the rows and columns.\n",
    "\n",
    "An **important point** to understand about the new line in the code above &mdash; line 17, `c.writerow( [college_name, location, url] )` &mdash; is that the square brackets *inside* the parentheses are **required** by the csv module's code. It expects to receive a Python list (which is like an array in JavaScript). If it doesn't, the CSV file will not be written correctly.\n",
    "\n",
    "So any time you WRITE a ROW to a CSV file here, you must write a *list* and NOT just variables separated by commas (which will not work).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We said we would get all the schools from all the lists, and we will\n",
    "\n",
    "What we've done is nice, but it will be even nicer when we get *all* the lists of schools that are still in operation (we will skip the closed schools). And we already have almost all the Python and Beautiful Soup code we need to get the job done.\n",
    "\n",
    "(If we are short on time for the live demonstration, we will stop here.)\n",
    "\n",
    "Recall that previously, we got a list of all the H3 headings. It's in the variable `h3_list`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h3_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all the complete H3 elements, with tags and everything. \n",
    "\n",
    "Let's look at just the *text* of the headings instead, using the same list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for head in h3_list:\n",
    "    if head.span:\n",
    "        print(head.span.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the page for missing data\n",
    "\n",
    "Always keep in mind that your code might not be getting all the items you want, and check the web page.\n",
    "\n",
    "When I check the Wikipedia page, I discover the list above is missing \"Trade/technical institutions.\" \n",
    "\n",
    "I use **Inspect** again and see that the H3 element there is a bit different from the others, so I try another way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for head in h3_list:\n",
    "    if head.find( 'span', {'class':'mw-headline'} ):\n",
    "        print( head.find( 'span', {'class':'mw-headline'} ).text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how that gave me a much better list!\n",
    "\n",
    "The BeautifulSoup pattern `object.find( 'element', {'class':'classname'} )` is very, very common and very, very useful. \n",
    "\n",
    "### Review basic Beautiful Soup commands\n",
    "\n",
    "`.find()` brings you only the **first** instance of the tag or class, even if there are others on the page. \n",
    "\n",
    "`.find_all()` brings you a Python **list** of ALL the elements of that kind on the page that have that class.\n",
    "\n",
    "`.text` strips the **text only** out of the element and gives it to you.\n",
    "\n",
    "When I examined the Wikipedia HTML carefully with **Inspect**, I realized that all the headings are inside a `span` tag that has `class=\"mw-headline` &mdash; *so that is what I used* to get the printed H3 headlines I want.\n",
    "\n",
    "Use Command-f (or Control-f on Windows) on the [BeautifulSoup documentation page](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to discover these tricks.\n",
    "\n",
    "Now I will manually create a Python list of the headings for the UL lists I want to scrape from this page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy/paste that nice text from above to create a new Python list \n",
    "# I removed 'Defunct public (county) colleges in Florida' and \n",
    "# 'Segregated junior colleges' because those schools are all closed \n",
    "\n",
    "headings = [\n",
    "    'State University System',\n",
    "    'Florida College System',\n",
    "    'Other public institutions',\n",
    "    'Religiously affiliated institutions',\n",
    "    'Trade/technical institutions',\n",
    "    'Other private institutions'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've checked the list carefully against the Wikipedia page. Every **list of schools** that I want to add to my CSV comes under one of those headings. I will use the headings the same way I used 'State University System' alone, [above](http://localhost:8888/notebooks/beginner-notebooks/completed/scraping_first_time.ipynb#Grab-a-list-of-schools-that-follows-a-certain-heading), to get ONE list of schools.\n",
    "\n",
    "I will use the `h3_list` as I did before &mdash; but now I will ALSO use the new `headings` list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in headings:\n",
    "    for head in h3_list:\n",
    "        text = head.find( 'span', {'class':'mw-headline'} ).text\n",
    "        if text == item:\n",
    "            the_ul = head.find_next('ul')\n",
    "            break\n",
    "    # show only the first LI in the UL - just to test this code \n",
    "    print( the_ul.li.text )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I printed only the text in the first LI in each list to test my code, and **I have a problem.** \n",
    "\n",
    "\"By state and in insular areas\" is not a school. Back on the Wikipedia page, I inspect again, and I find that the heading \"Other private institutions\" is followed (in the HTML) by a table, and *inside the table* there is a UL I do not want.\n",
    "\n",
    "This is a poor page layout. We can't help that. \n",
    "\n",
    "This is a typical scraping problem, and we can solve it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in headings:\n",
    "    for head in h3_list:\n",
    "        text = head.find( 'span', {'class':'mw-headline'} ).text\n",
    "        if text == item:\n",
    "            the_ul = head.find_next_sibling('ul')\n",
    "            break\n",
    "    print( the_ul.li.text )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using siblings in Beautiful Soup\n",
    "\n",
    "Siblings are tricky to work with, and they don't always act like you would expect. Learn more about them in the [BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "`head.find_next_sibling('ul')` means \"get the first sibling that is a UL element following this head.\"\n",
    "\n",
    "Since we now know we can successfully get all the LI elements, let's try to add in the code that gets the location and the URL too. (We continue testing without writing a CSV until we *know* we can write the CSV file cleanly.)\n",
    "\n",
    "We continue *printing* because we are still *testing.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in headings:\n",
    "    for head in h3_list:\n",
    "        text = head.find( 'span', {'class':'mw-headline'} ).text\n",
    "        if text == item:\n",
    "            the_ul = head.find_next_sibling('ul')\n",
    "            break\n",
    "    \n",
    "    # get all schools under ONE heading\n",
    "    schools = the_ul.find_all('li')\n",
    "\n",
    "    # remember this? we used it above, earlier \n",
    "    for li in schools:\n",
    "        a_list = li.find_all('a')\n",
    "        college_name = a_list[0].text\n",
    "        location = a_list[1].text\n",
    "        url = 'https://en.wikipedia.org' + a_list[0]['href']\n",
    "        print(college_name, location, url, item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good, so let's insert that whole chunk of code *into* the CSV-writing code and give it a try.\n",
    "\n",
    "## Make the final, complete CSV of all schools\n",
    "\n",
    "We'll use the same CSV code we used above. Remember, we don't need to `import csv` again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and open a new file for writing\n",
    "csvfile = open(\"florida_colleges.csv\", 'w', newline='', encoding='utf-8')\n",
    "\n",
    "# make a new variable, c, for Python's CSV writer object \n",
    "c = csv.writer(csvfile)\n",
    "\n",
    "# write your header row \n",
    "c.writerow( ['school', 'location', 'url', 'type'] )\n",
    "\n",
    "# write all the schools into rows\n",
    "for item in headings:\n",
    "    for head in h3_list:\n",
    "        text = head.find( 'span', {'class':'mw-headline'} ).text\n",
    "        if text == item:\n",
    "            the_ul = head.find_next_sibling('ul')\n",
    "            break\n",
    "    \n",
    "    # get all schools under ONE heading\n",
    "    schools = the_ul.find_all('li')\n",
    " \n",
    "    for li in schools:\n",
    "        a_list = li.find_all('a')\n",
    "        college_name = a_list[0].text\n",
    "        location = a_list[1].text\n",
    "        url = 'https://en.wikipedia.org' + a_list[0]['href']\n",
    "        \n",
    "        # write one row into the CSV\n",
    "        c.writerow( [college_name, location, url, item] )\n",
    "\n",
    "# close and save the file\n",
    "csvfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it works! You have a clean CSV with 120+ schools, and you could use it to make a map, a database, a mailing list (that would require more scraping on other pages), etc. You can sort the schools alphabetically or by location and still know which type of schools they are because of the \"type\" column. Some schools might not be real (it is Wikipedia, after all), so some more human intelligence would need to be applied to the CSV before it could be used with confidence.\n",
    "\n",
    "**Note** that *only the code in the final cell* is needed to make this CSV. Many of the cells above that were testing and working to get to this point. \n",
    "\n",
    "Also needed were:\n",
    "\n",
    "* import statements for Beautiful Soup, Requests, and csv \n",
    "* the cell in which we opened the Wikipedia page and copied all its HTML into a variable named `soup`\n",
    "* create `h3_list`\n",
    "* create `headings`\n",
    "\n",
    "**Note also** that the list of URLs (which might need some cleanup) could be used to further scrape data from each college's Wikipedia page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
