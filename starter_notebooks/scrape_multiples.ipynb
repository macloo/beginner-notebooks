{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: How to scrape multiple things from multiple pages\n",
    "\n",
    "This example uses the website [Box Office Mojo](https://www.boxofficemojo.com/).\n",
    "\n",
    "The goal is to scrape info about the five top-grossing movies for each year, for 10 years. I want the title and rank of the movie, and also, how much money it grossed at the box office. In the end I will put the scraped data into a CSV file. This is a small-scale project to demonstrate how a larger-scale project using various other websites (instead of Box Office Mojo) might be accomplished.\n",
    "\n",
    "This demo uses:\n",
    "\n",
    "* Python 3\n",
    "* Jupyter Notebook\n",
    "* [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [Requests](https://requests.kennethreitz.org/en/master/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Python libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin to explore ONE page - load the page and capture all of its HTML in a variable named `soup` \n",
    "url = 'https://www.boxofficemojo.com/yearly/chart/?yr=2019'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can start trying to get the data you want, you will *explore* the HTML of the selected web page, using Chrome Developer Tools.\n",
    "\n",
    "**Step 1:** Right-click on the exact line or element you wish to capture (scrape). A menu pops up. Select **Inspect.**\n",
    "\n",
    "The Elements inspection pane will open. If this is on the side (instead of at bottom, as shown), you can move it by clicking the kebab menu icon in the upper right corner, beside the X.\n",
    "\n",
    "**Step 2:** Examine the HTML elements that hold the data you want to scrape. You'll use these element names to scrape the contents. If you don't know much about HTML, [this brief guide](http://bit.ly/mm-htmltags) will help.\n",
    "\n",
    "The **circled area** in the HTML shows us that the data we want is contained in a `table` element. HTML tables have rows &mdash; in `tr` elements. The rows contain cells &mdash; in `td` elements. The start of a row, and some cells, can be seen in the **boxed area** above.\n",
    "\n",
    "There are various ways to scrape HTML tables other than what is demonstrated here. Our goal here is to demonstrate a scraping method that can work for more than just tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I discover the data I want is in an HTML table with no class or ID \n",
    "# I capture all the tables on the page like this - in a Python list named tables -\n",
    "tables = soup.find_all( 'table' )\n",
    "\n",
    "# then I print the number of how many tables are in that list \n",
    "print(len(tables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to test a few numbers before I got the correct `tables[]` and `rows[]` numbers you see below. I have deleted those testing cells from this notebook. It's important for beginners to understand that scraping requires a lot of trial-and-error work. It would be cumbersome to keep that work in the notebook, so it is gone. However, you will need to *do* that work before you discover how to get the data you desire from a page.\n",
    "\n",
    "The number in square brackets &mdash; `[6]` &mdash; is the result of my trial-and-error testing. I just kept changing the number in `tables[ ]` and printing until I found the right table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I capture all the rows in that table like this - in a Python list named rows -\n",
    "rows = tables[6].find_all('tr')\n",
    "\n",
    "# Here is some of the testing needed to find the correct data row\n",
    "print(len(rows))\n",
    "print(rows[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above: There are 106 rows in my list, named `rows`. I can see on the page that the top row contains column headings, which I do not want to scrape. Some more (deleted) testing showed me that `rows[0]` and `rows[1]` contained data I do not want. Therefore I printed `rows[2]` to have a look at its contents.\n",
    "\n",
    "The contents are a lot of HTML elements all mashed together. Can you see the `td` that contains the first movie title?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see whether I can get the first movie title in the first row\n",
    "cells = rows[2].find_all('td')\n",
    "title = cells[1].text\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Python lists, we access items in a list by their index, which is the number you see inside square brackets. `cells[1].text` is the text inside the *second* item in the list named `cells`. The first item in any list has the index `0`.\n",
    "\n",
    "To access only the cells in the first row of data, I made a new list, named `cells`.\n",
    "\n",
    "My next test is to determine whether I can cleanly get the contents I want from the first five rows in this table. I use a Python for-loop to do this. Using `range(2, 7)` will bring me the rows with the *indexes* 2 through 6 (7 is the endpoint in the range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 5 movies on this page - I know the first row is [2]\n",
    "for i in range(2, 7):\n",
    "    cells = rows[i].find_all('td')\n",
    "    title = cells[1].text\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My test was successful! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would like to get the total gross number also\n",
    "for i in range(2, 7):\n",
    "    cells = rows[i].find_all('td')\n",
    "    gross = cells[3].text\n",
    "    print(gross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing each individual thing you want is easier to debug than trying to get all the things in one big chunk of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next I want to get rank (1-5), title and gross all on one line\n",
    "for i in range(2, 7):\n",
    "    cells = rows[i].find_all('td')\n",
    "    print(cells[0].text, cells[1].text, cells[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to do this for 10 years, ending with 2019\n",
    "# first create a list of the years I want\n",
    "years = []\n",
    "start = 2019\n",
    "for i in range(0, 10):\n",
    "    years.append(start - i)\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, I could have *typed* out that list, named `years`, by hand, but a for-loop created the list for me. It's nice to use `range()` to do things like this.\n",
    "\n",
    "**Feel free to try other years!** Box Office Mojo charts go back to 1980, so you could start your list (previous cell) with 1989."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a base url so I can open each year's page\n",
    "base_url = 'https://www.boxofficemojo.com/yearly/chart/?yr='\n",
    "# test it\n",
    "# print(base_url + years[0]) -- ERROR \n",
    "# years[0] is an integer, so I must convert it to a string, with str()\n",
    "print( base_url + str(years[0]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am working toward making a loop that will open each web page I want (one for each year) and scrape from that page the top five movies and their gross earnings. To do this, I will combine the `base_url` text with the year. That will give me 10 URLs, one for each year.\n",
    "\n",
    "Now that my testing process is (mosty) done, I can attempt to make the complete set of instructions to scrape all 10 pages &mdash;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all necessary pieces from above to make a loop that gets top 5 movies \n",
    "# for each of the 10 years\n",
    "for year in years:\n",
    "    url = base_url + str(year)\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    tables = soup.find_all( 'table' )\n",
    "    rows = tables[6].find_all('tr')\n",
    "    for i in range(2, 7):\n",
    "        cells = rows[i].find_all('td')\n",
    "        print(cells[0].text, cells[1].text, cells[3].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! However, looking at this, I realize that each line needs to include the year as well. \n",
    "\n",
    "I also realize I should clean the gross so it is a pure integer. That is a common data-cleaning task, but I will do a test before I add it to my code.\n",
    "\n",
    "[How `strip()` works](https://docs.python.org/3/library/stdtypes.html#str.strip)\n",
    "\n",
    "[How `replace()` works](https://docs.python.org/3/library/stdtypes.html#str.replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test making a pure integer from the gross - using .strip() and .replace() chained together - \n",
    "num = '$293,004,164'\n",
    "print(num.strip('$').replace(',', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I make another attempt at the complete set of instructions to scrape all 10 pages. This time I am also going to write the year into the line. Instead of using all 10 years (because this is just a test), I create a tiny array with just two years in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniyears = [2017, 2014]\n",
    "for year in miniyears:\n",
    "    url = base_url + str(year)\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    tables = soup.find_all( 'table' )\n",
    "    rows = tables[6].find_all('tr')\n",
    "    for i in range(2, 7):\n",
    "        cells = rows[i].find_all('td')\n",
    "        gross = cells[3].text.strip('$').replace(',', '')\n",
    "        print(year, cells[0].text, cells[1].text, gross)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am confident this code will work with all 10 years. But before I run it with `years`, I want to be sure to save the data in a new CSV file. This uses some standard Python code that always works fine, so I do not bother to test it. I just add it in. This will be my final cell in the notebook. \n",
    "\n",
    "After it runs, I will have a new file named `movies.csv` in the same folder as this notebook, and it will contain a header row and 50 rows of movie data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save data into a csv, we must import -\n",
    "\n",
    "import csv\n",
    "\n",
    "# open a new file for writing -\n",
    "csvfile = open(\"movies.csv\", 'w', newline='', encoding='utf-8')\n",
    "\n",
    "# make a new variable, c, for Python's CSV writer object -\n",
    "c = csv.writer(csvfile)\n",
    "\n",
    "# write custom header row to csv\n",
    "c.writerow( ['year', 'rank', 'title', 'gross'] )\n",
    "\n",
    "# modified code that was already tested, from above \n",
    "for year in years:\n",
    "    url = base_url + str(year)\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    tables = soup.find_all( 'table' )\n",
    "    rows = tables[6].find_all('tr')\n",
    "    for i in range(2, 7):\n",
    "        cells = rows[i].find_all('td')\n",
    "        gross = cells[3].text.strip('$').replace(',', '')\n",
    "        # print(year, cells[0].text, cells[1].text, gross)\n",
    "        # instead of printing, I need to make a list and write that list to the CSV as one row\n",
    "        c.writerow( [year, cells[0].text, cells[1].text, gross] )\n",
    "\n",
    "# close the csv file and save it\n",
    "csvfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a CSV file, named `movies.csv`, that has 51 rows: the header row plus 5 movies for each year from 2010 through 2019. It has four columns: year, rank, title, and gross.\n",
    "\n",
    "Note that **only the final cell above** is needed to create this CSV, by scraping 10 separate web pages. Everything *above* the final cell above is just instruction, demonstration. It is intended to show the problem-solving you need to go through to get to a desired scraping result.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
